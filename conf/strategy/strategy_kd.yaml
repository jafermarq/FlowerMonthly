# Demonstrates a simplified setup of client-side knowledge distillation. Different from other typical
# FL examples, here the server communicates two models: (1) a teacher model that's pre-trained and (2) a
# student network that's learned in a federated fashion by distilling on the client's side.

# Let's pass first (order doesn't matter) the arguments unique to our custom strategy
_target_: src.strategy.CustomFedAvgWithKD
teacher:
  _target_: src.models.ResNet18 # let's use a ResNet-18 as our teach (because why not)
  num_classes: 10
kd_config:
  teacher_pretrain:
    batch_size: 32
    optim:
      _target_: torch.optim.SGD
      lr: 0.1
      momentum: 0.9
    num_batches: 50 # let's limit how many batches of data are use for training the teacher.
  student_train:
    temperature: 2
    alpha: 0.5
# We pass the usual arguments needed for a strategy (in this case ours inherits from FedAvg)
fraction_fit: 0.0001
fraction_evaluate: 0.0
min_fit_clients: ${server.clients_per_round}
min_available_clients: ${server.pool}
on_fit_config_fn:
  _target_: src.server.gen_fit_config # function to call eventually
  fit_cfg: ${server.fit_cfg}
  